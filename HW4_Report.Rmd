---
title: "ISYE6501 HW4"
author: "Keh-Harng Feng"
date: "June 6, 2017"
output: 
  bookdown::html_document2:
    fig_caption: TRUE
    toc: FALSE
urlcolor: blue
---
```{r setup, include=FALSE}
library('knitr')
library('rpart')
library('randomForest')
library('caret')
library('parallel')

opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, tidy = TRUE, cache = TRUE)
options(digits = 4)


```

## Preface
This is a reproducible report with most of the codes doing the heavy lifting hidden in the background. **All codes are available to be audited.** You can download the source code of the report by [clicking here](https://github.com/fengkehh/ISYE6501WK4/blob/master/HW4_Report.Rmd). As a general rule of thumb you should NOT run any downloaded R scripts from an untrusted source on your computer without understanding the source code first.

# Question 1
**Using the same crime data set as in Homework 3 Question 4, apply Principal Component Analysis and then create a regression model using the first 4 principal components. Specify your new model in terms of the original variables (not the principal components), and compare its quality to that of your solution to Homework 3 Question 4. You can use the R function prcomp for PCA. (Note that to first scale the data, you can include scale. = TRUE to scale as part of the PCA function.)**

## Model Consturction
```{r}
set.seed(1)

q1_data <- read.table('uscrime.txt', header = TRUE)

q1_data$So <- factor(q1_data$So)

n <- nrow(q1_data)

inTrain <- sample(1:n, size = ceiling(n*0.9))

data.train <- q1_data[inTrain,]
data.test <- q1_data[-inTrain,]

# Stupid hack to make sure test set factors have the same level as training set.
data.test <- rbind(data.train[1, ], data.test)
data.test <- data.test[-1,]

pca_comps <- prcomp(formula = ~. - Crime - So,  data = data.train, center = TRUE, scale = TRUE)

predict_pcomps <- function(pcomps, data) {
    if ('Crime' %in% names(data)) {
        df <- data.frame(predict(pcomps, data), So = data$So, Crime = data$Crime)
    } else {
        df <- data.frame(predict(pcomps, data), So = data$So)
    }
    return(df)
}

data.train.pcomps <- predict_pcomps(pca_comps, data.train)

model <- lm(Crime ~ PC1 + PC2 + PC3 + PC4 + So, data = data.train.pcomps)

data.test.pcomps <- predict_pcomps(pca_comps, data.test)

pred <- predict(model, data.test.pcomps)

mse <- ModelMetrics::mse(data.test$Crime, pred)

data.new <- data.frame(M = 14, So = factor(0, levels = c(0, 1)), Ed = 10, Po1 = 12, Po2 = 15.5, LF = 0.64, M.F = 94, Pop = 150, NW = 1.1, U1 = 0.12, U2 = 3.6, Wealth = 3200, Ineq = 20.1, Prob = 0.04, Time = 39.0)

data.new.pcomps <- predict_pcomps(pca_comps, data.new)

newpred <- predict(model, data.new.pcomps)
```

The data set is split into a test (~10%) and training (~90%) with the same random seed used in my HW3 report. Principle components are computed for all of the predictors **except** `So` because it is categorical.

Figure \@ref(fig:varexp) shows the cumulative amount of variance explained by the first four principle components. Without `So` they already explain over 99% of the variance.

```{r varexp, fig.cap = 'Cumulative porportion of variance explained by the principle components.'}
plot(summary(pca_comps)$importance[3,], xlab = 'Principle Component Index', ylab = 'Cumulative Porportion of Variance Explained')
```

The final model is trained using the first four principle components plus `So` as the predictors. The final model using principle components contains the following coefficients:

```{r}
model$coefficients
```

Define a 4x1 column vector $\bar{M}$ containing the model coefficients *except* the intercept and the coefficient for `So`. The equation of the line constructed by linear regression can be written in terms of the original predictors as follows:

\begin{equation}
\bar{Y} = (Intercept) + \bar{X} \times \hat{P} \times \bar{M} + X_{So}M_{So}
(\#eq:lincomb)
\end{equation}

where

$\bar{X}$: 1x14 vector of centered & scaled predictors used in the principle components.

$\hat{P}$: 14x4 rotation matrix of the first 4 principle components.

$X_{So}$: The `So` predictor.

$M_{So}$: scalar model `So` coefficient.

\pagebreak

```{r}
coefs <- pca_comps$rotation[,1:4] %*% t(t(model$coefficients[2:5])) / pca_comps$scale 

varnames <- rownames(coefs)

coefs <- setNames(as.vector(coefs), varnames)


equation <- function(varname, coef, center) {
    
    str = paste(coef, '*(', varname, ' - ', center, ')', sep = '') 
    return(str)
}

So_str <- paste(model$coefficients[6], '*So', sep = '')
equation_str <- paste('Crime = ', model$coefficients[1], paste(c(mapply(FUN = equation, varnames, coefs, pca_comps$center), So_str), collapse = ' + '), sep = '')
```

Writing out the model using the original predictors, we have:

$`r equation_str`$

The table below shows the results computed from the above equation (using regular predictors) and the predictions using predict() on the PCA transformed test set side by side:

```{r}
my_predict_func <- function(coefs, center, newdata) {
    resp <- rep(0, nrow(newdata))
    
    for (i in 1:nrow(newdata)) {
        datapoint <- newdata[i,]
        
        for (var in varnames) {
            resp[i] <- resp[i] + coefs[var]*(datapoint[[var]] - center[var])
        }
    
        resp[i] <- resp[i] + model$coefficients[6]*as.numeric(paste(datapoint$So)) + model$coefficients[1]
    
    }
    
    return(resp)
    
}

my_pred <- my_predict_func(coefs, pca_comps$center, data.test)

df <- data.frame(Equation = my_pred, PCA = pred)
rownames(df) <- c()

kable(df)
```

The equation is indeed correct.

## Model Performance

The model using the first four principle components achieved a MSE of $`r mse`$ on the test set. The model I built in HW3 using the same training set was able to achieve a MSE of 1.3309e4 on the same test set. **The PC model has worse performance.** It should be noted however that my HW3 model was built with extensive feature selection along with data transformation assisted by exploratory data analysis. It is also possible that my HW3 model was overfitted (especially since the sample was so small and there was a bit of test set contamination due to transformation being carried out prior to training/test split.)

# Question 2
**Using the same crime data set as in Homework 3 Question 4, find the best model you can using (a) a regression tree model, and (b) a random forest model. In R, you can use the tree package or the rpart package, and the randomForest package. For each model, describe one or two qualitative takeaways you get from analyzing the results (i.e., donâ€™t just stop when you have a good model, but interpret it too).**

## Part A: Regression Tree
```{r}
set.seed(123)
trCon <- trainControl(method = 'cv', number = 10)
tree <- train(Crime ~., data = data.train, method = 'rpart', trControl = trCon)
```

A regression tree model is built using `rpart` in conjunction with `train()` from `caret`. Automatic tuning of the `cp` parameter was done using cross-validation. No transformation on the data was done prior to model training since tree algorithims are generally not sensitive to things such as predictor/response distributions. Model information is shown below:

```{r}
tree
```


## Part B: Random Forest
A bit of parameter tuning is done to create a model random forest. The out-of-sample MSE is estimated using 10-fold CV for forests ranging from 50 trees to 5000 in increment of 50. The resulting average MSE is plotted in Figure \@ref(fig:msetrees).

```{r}
set.seed(123)

inVal <- sample(1:nrow(data.train), size = ceiling(n*0.1))

forest <- randomForest(Crime ~ ., data = data.train, ntree = 1000)

tree_pred <- predict(tree, data.test)
forest_pred <- predict(forest, data.test)

#ModelMetrics::mse(data.test$Crime, tree_pred)

#ModelMetrics::mse(data.test$Crime, forest_pred)

ntrees <- seq(from = 50, to = 5000, by = 50)
mse_vec <- rep(0, length(ntrees))

folds <- createFolds(data.train$Crime, k = 10)

# ntree mse plot

test_mse <- function(fold, num_trees, data){
    data.fold <- data[fold,]
    data.outside <- data[-fold,]
    forest_test <- randomForest::randomForest(Crime ~ ., data = data.outside, ntree = num_trees)  
    mse_val <- ModelMetrics::mse(data.fold$Crime, predict(forest_test, data.fold))
} 

# Set up parallel processing clusters
cl <- makePSOCKcluster(4)

for (i in seq_along(ntrees)) {
    
    
    
    mse_test <- parSapply(cl = cl, X = folds, FUN = test_mse, 
                          num_trees = ntrees[i], data = data.train)
    
    mse_vec[i] <- mean(mse_test)
}

stopCluster(cl)
```

```{r msetrees, fig.cap = 'Estimated OOS MSE vs Number of Trees in Forest'}
plot(ntrees, mse_vec, xlab = 'Number of Trees', ylab = 'MSE Estimation (10-fold CV)')
```

While overall there is no discernible decrease to the estimated MSE as the number of trees increases, the variance in MSE does become smaller. This seems to indicate that the more trees a random forest has, the more stable it is in making prediction. The highest number of trees tested, ntrees = 5000, is chosen to build the final random forest model.

## Model Comparison & Performance Evaluation
