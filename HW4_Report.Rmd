---
title: "ISYE6501 HW4"
author: "Keh-Harng Feng"
date: "June 6, 2017"
output: 
  bookdown::pdf_book:
    fig_caption: TRUE
    toc: FALSE
urlcolor: blue
---
```{r setup, include=FALSE}
library('knitr')
opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, tidy = TRUE, cache = TRUE)
options(digits = 4)
```

## Preface
This is a reproducible report with most of the codes doing the heavy lifting hidden in the background. **All codes are available to be audited.** You can download the source code of the report by [clicking here](https://github.com/fengkehh/ISYE6501WK4/blob/master/HW4_Report.Rmd). As a general rule of thumb you should NOT run any downloaded R scripts from an untrusted source on your computer without understanding the source code first.

# Question 1
**Using the same crime data set as in Homework 3 Question 4, apply Principal Component Analysis and then create a regression model using the first 4 principal components. Specify your new model in terms of the original variables (not the principal components), and compare its quality to that of your solution to Homework 3 Question 4. You can use the R function prcomp for PCA. (Note that to first scale the data, you can include scale. = TRUE to scale as part of the PCA function.)**

## Model Consturction
```{r}
set.seed(1)

q1_data <- read.table('uscrime.txt', header = TRUE)

n <- nrow(q1_data)

inTrain <- sample(1:n, size = ceiling(n*0.9))

data.train <- q1_data[inTrain,]
data.test <- q1_data[-inTrain,]

pca_comps <- prcomp(formula = ~. - Crime - So,  data = data.train, center = TRUE, scale = TRUE)

predict_pcomps <- function(pcomps, data) {
    if ('Crime' %in% names(data)) {
        df <- data.frame(predict(pcomps, data), So = data$So, Crime = data$Crime)
    } else {
        df <- data.frame(predict(pcomps, data), So = data$So)
    }
    return(df)
}

data.train.pcomps <- predict_pcomps(pca_comps, data.train)

model <- lm(Crime ~ PC1 + PC2 + PC3 + PC4 + factor(So), data = data.train.pcomps)

data.test.pcomps <- predict_pcomps(pca_comps, data.test)

pred <- predict(model, data.test.pcomps)

mse <- ModelMetrics::mse(data.test$Crime, pred)

data.new <- data.frame(M = 14, So = factor(0, levels = c(0, 1)), Ed = 10, Po1 = 12, Po2 = 15.5, LF = 0.64, M.F = 94, Pop = 150, NW = 1.1, U1 = 0.12, U2 = 3.6, Wealth = 3200, Ineq = 20.1, Prob = 0.04, Time = 39.0)

data.new.pcomps <- predict_pcomps(pca_comps, data.new)

newpred <- predict(model, data.new.pcomps)
```

The data set is split into a test (~10%) and training (~90%) with the same random seed used in my HW3 report. Principle components are computed for all of the predictors **except** `So` because it is categorical.

Figure \@ref(fig:varexp) shows the cumulative amount of variance explained by the first four principle components. Without `So` they already explain over 99% of the variance.

```{r varexp, fig.cap = 'Cumulative porportion of variance explained by the principle components.'}
plot(summary(pca_comps)$importance[3,], xlab = 'Principle Component Index', ylab = 'Cumulative Porportion of Variance Explained')
```

The final model is trained using the first four principle components plus `So` as the predictors. The final model using principle components contains the following coefficients:

```{r}
model$coefficients
```

Define a 4x1 column vector $\bar{M}$ containing the model coefficients *except* the intercept and the coefficient for `So`. The equation of the line constructed by linear regression can be written in terms of the original predictors as follows:

\begin{equation}
\bar{Y} = (Intercept) + \bar{X} \times \hat{P} \times \bar{M} + X_{So}M_{So}
(\#eq:lincomb)
\end{equation}

where

$\bar{X}$: 1x14 vector of centered & scaled predictors used in the principle components.

$\hat{P}$: 14x4 rotation matrix of the first 4 principle components.

$X_{So}$: The `So` predictor.

$M_{So}$: scalar model `So` coefficient.

\pagebreak

```{r}
coefs <- pca_comps$rotation[,1:4] %*% t(t(model$coefficients[2:5])) / pca_comps$scale 

varnames <- rownames(coefs)

coefs <- setNames(as.vector(coefs), varnames)


equation <- function(varname, coef, center) {
    
    str = paste(coef, '*(', varname, ' - ', center, ')', sep = '') 
    return(str)
}

So_str <- paste(model$coefficients[6], '*So', sep = '')
equation_str <- paste('Crime = ', model$coefficients[1], paste(c(mapply(FUN = equation, varnames, coefs, pca_comps$center), So_str), collapse = ' + '), sep = '')
```

Writing out the model using the original predictors, we have:

$`r equation_str`$

The table below shows the results computed from the above equation (using regular predictors) and the predictions using predict() on the PCA transformed test set side by side:

```{r}
my_predict_func <- function(coefs, center, newdata) {
    resp <- rep(0, nrow(newdata))
    
    for (i in 1:nrow(newdata)) {
        datapoint <- newdata[i,]
        
        for (var in varnames) {
            resp[i] <- resp[i] + coefs[var]*(datapoint[[var]] - center[var])
        }
    
        resp[i] <- resp[i] + model$coefficients[6]*datapoint$So + model$coefficients[1]
    
    }
    
    return(resp)
    
}

my_pred <- my_predict_func(coefs, pca_comps$center, data.test)

df <- data.frame(Equation = my_pred, PCA = pred)
rownames(df) <- c()

kable(df)
```

The equation is indeed correct.

## Model Performance