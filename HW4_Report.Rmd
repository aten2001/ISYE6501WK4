---
title: "ISYE6501 HW4"
author: "Keh-Harng Feng"
date: "June 6, 2017"
header-includes:
    - \usepackage{placeins}
output: 
  bookdown::pdf_book:
    fig_caption: TRUE
    toc: FALSE
urlcolor: blue
---
```{r setup, include=FALSE}
library('knitr')
library('rpart')
library('randomForest')
library('caret')
library('parallel')
library('rpart.plot')
library('MASS')

opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, tidy = TRUE, cache = TRUE)
options(digits = 4)


```

## Preface
This is a reproducible report with most of the codes doing the heavy lifting hidden in the background. Someone wondered why I don't show my code on my report. The reason is simple: the code chunks are often VERY long and the information you need as a reviewer is in the report, not the code. As always, it is still available for completeness and reproducibility. You can download the source code of the report by [clicking here](https://github.com/fengkehh/ISYE6501WK4/blob/master/HW4_Report.Rmd).  

# Question 1
**Using the same crime data set as in Homework 3 Question 4, apply Principal Component Analysis and then create a regression model using the first 4 principal components. Specify your new model in terms of the original variables (not the principal components), and compare its quality to that of your solution to Homework 3 Question 4. You can use the R function prcomp for PCA. (Note that to first scale the data, you can include scale. = TRUE to scale as part of the PCA function.)**

## Model Consturction
```{r}
set.seed(1)

q1_data <- read.table('uscrime.txt', header = TRUE)

q1_data$So <- factor(q1_data$So)

n <- nrow(q1_data)

inTrain <- sample(1:n, size = ceiling(n*0.9))

data.train <- q1_data[inTrain,]
data.test <- q1_data[-inTrain,]

# Stupid hack to make sure test set factors have the same level as training set.
data.test <- rbind(data.train[1, ], data.test)
data.test <- data.test[-1,]

pca_comps <- prcomp(formula = ~. - Crime - So,  data = data.train, center = TRUE, scale = TRUE)

predict_pcomps <- function(pcomps, data) {
    if ('Crime' %in% names(data)) {
        df <- data.frame(predict(pcomps, data), So = data$So, Crime = data$Crime)
    } else {
        df <- data.frame(predict(pcomps, data), So = data$So)
    }
    return(df)
}

data.train.pcomps <- predict_pcomps(pca_comps, data.train)

model <- lm(Crime ~ PC1 + PC2 + PC3 + PC4 + So, data = data.train.pcomps)

data.test.pcomps <- predict_pcomps(pca_comps, data.test)

pred <- predict(model, data.test.pcomps)

q1_mse <- ModelMetrics::mse(data.test$Crime, pred)
```

The data set is split into a test (~10%) and training (~90%) with the same random seed used in my [HW3 report](https://github.com/fengkehh/ISYE6501WK3/blob/master/HW3_Report.pdf). Principle components are computed for all of the predictors **except** `So` because it is categorical.

Figure \@ref(fig:varexp) shows the cumulative amount of variance explained by the first four principle components. Without `So` they already explain over 99% of the variance.

```{r varexp, fig.cap = 'Cumulative porportion of variance explained by the principle components.'}
plot(summary(pca_comps)$importance[3,], xlab = 'Principle Component Index', ylab = 'Cumulative Porportion of Variance Explained')
```

The final model is trained using the first four principle components plus `So` as the predictors. The final model using principle components contains the following coefficients:

```{r}
model$coefficients
```

Define a 4x1 column vector $\bar{M}$ containing the model coefficients *except* the intercept and the coefficient for `So`. The equation of the line constructed by linear regression can be written in terms of the original predictors as follows:

\begin{equation}
\bar{Y} = (Intercept) + \bar{X} \times \hat{P} \times \bar{M} + X_{So}M_{So}
(\#eq:lincomb)
\end{equation}

where

$\bar{X}$: 1x14 vector of centered & scaled predictors used in the principle components.

$\hat{P}$: 14x4 rotation matrix of the first 4 principle components.

$X_{So}$: The `So` predictor.

$M_{So}$: scalar model `So` coefficient.

\pagebreak

```{r}
coefs <- pca_comps$rotation[,1:4] %*% t(t(model$coefficients[2:5])) / pca_comps$scale 

varnames <- rownames(coefs)

coefs <- setNames(as.vector(coefs), varnames)


equation <- function(varname, coef, center) {
    
    str = paste(coef, '*(', varname, ' - ', center, ')', sep = '') 
    return(str)
}

So_str <- paste(model$coefficients[6], '*So', sep = '')
equation_str <- paste('Crime = ', model$coefficients[1], paste(c(mapply(FUN = equation, varnames, coefs, pca_comps$center), So_str), collapse = ' + '), sep = '')
```

Writing out the model using the original predictors, we have:

$`r equation_str`$

Table \@ref(tab:eq-pca) shows the results computed from the above equation using regular predictors directly from the test set and the predictions using `predict()` on the PCA model with the predictors in the test set transformed accordingly. The equation is indeed correct.

\FloatBarrier

```{r eq-pca}
my_predict_func <- function(coefs, center, newdata) {
    resp <- rep(0, nrow(newdata))
    
    for (i in 1:nrow(newdata)) {
        datapoint <- newdata[i,]
        
        for (var in varnames) {
            resp[i] <- resp[i] + coefs[var]*(datapoint[[var]] - center[var])
        }
    
        resp[i] <- resp[i] + model$coefficients[6]*as.numeric(paste(datapoint$So)) + model$coefficients[1]
    
    }
    
    return(resp)
    
}

my_pred <- my_predict_func(coefs, pca_comps$center, data.test)

df <- data.frame(Equation = my_pred, PCA = pred)
rownames(df) <- c()

kable(df, caption = 'Prediction values using direct equation vs prediction values using PCA model.')
```

## Model Performance

The model using the first four principle components achieved a MSE of $`r q1_mse`$ on the test set. The model I built in HW3 using the same training set was able to achieve a MSE of 1.3309e4 on the same test set. **The PC model has worse performance.** It should be noted however that my HW3 model was built with extensive feature selection along with data transformation assisted by exploratory data analysis. It is also possible that my HW3 model was overfitted (especially since the sample was so small and there was a bit of test set contamination due to transformation being carried out prior to training/test split - a mistake that won't be repeated here.)

\pagebreak

# Question 2
**Using the same crime data set as in Homework 3 Question 4, find the best model you can using (a) a regression tree model, and (b) a random forest model. In R, you can use the tree package or the rpart package, and the randomForest package. For each model, describe one or two qualitative takeaways you get from analyzing the results (i.e., don’t just stop when you have a good model, but interpret it too).**

## Part A: Regression Tree
```{r}
set.seed(123)
trCon <- trainControl(method = 'cv', number = 10)
tree <- train(Crime ~., data = data.train, method = 'rpart', trControl = trCon, parms = list(method = 'anova'))
tree_pred <- predict(tree, data.test)
tree_mse <- ModelMetrics::mse(data.test$Crime, tree_pred)
```

As usual, the data is split into the same training set and test set as before. A regression tree model is built using `rpart` in conjunction with `train()` from `caret` on the training set. Automatic tuning of the `cp` parameter was done using cross-validation. No transformation on the data was done prior to model training since tree algorithims are generally not sensitive to things such as predictor/response distributions. Model information is shown below:

```{r}
tree
```

A graphical representation of the tree is shown in Figure \@ref(fig:rpartplot)

```{r rpartplot, fig.cap = 'The regression tree model.'}
rpart.plot(tree$finalModel)
```

Incredibly the final tree model only makes use of ONE predictor, `Po2`. It can only make TWO different predictions: 666 or 1154 for the crime rate. Decisions are made by checking the `Po2` predictor values against 7.2. Its performance will be evaluated later on the test set.

## Part B: Random Forest
A bit of parameter tuning is done to create a model random forest. The out-of-sample MSE is estimated using 10-fold CV for forests ranging from 50 trees to 5000 in increment of 50. The resulting average MSE is plotted in Figure \@ref(fig:msetrees).

```{r}
set.seed(123)

ntrees <- seq(from = 50, to = 5000, by = 50)
mse_vec <- rep(0, length(ntrees))

folds <- createFolds(data.train$Crime, k = 10)

# ntree mse plot

test_mse <- function(fold, num_trees, data){
    data.fold <- data[fold,]
    data.outside <- data[-fold,]
    forest_test <- randomForest::randomForest(Crime ~ ., data = data.outside, ntree = num_trees)  
    mse_val <- ModelMetrics::mse(data.fold$Crime, predict(forest_test, data.fold))
    return(mse_val)
} 

# Set up parallel processing clusters
cl <- makePSOCKcluster(4)

for (i in seq_along(ntrees)) {
    
    
    
    mse_test <- parSapply(cl = cl, X = folds, FUN = test_mse, 
                          num_trees = ntrees[i], data = data.train)
    
    mse_vec[i] <- mean(mse_test)
}

stopCluster(cl)
```

```{r msetrees, fig.cap = 'Estimated OOS MSE vs Number of Trees in Forest'}
plot(ntrees, mse_vec, xlab = 'Number of Trees', ylab = 'MSE Estimation (10-fold CV)')
```

While overall there is no discernible decrease to the average of estimated MSE as the number of trees increases, the variance in MSE does become smaller. This seems to indicate that the more trees a random forest has, the more stable it is in making prediction. The highest number of trees tested, ntrees = 5000, is chosen to build the final random forest model.

```{r}
set.seed(123)
forest <- randomForest(Crime ~ ., data = data.train, ntree = 5000)
forest_pred <- predict(forest, data.test)
forest_mse <- ModelMetrics::mse(data.test$Crime, forest_pred)
```

Unfortunately, unlike regression trees there are no simple ways to visualize the forest effectively. Interpretation of random forest models is always a difficult if not down right impossible task due to the aggregation of many trees together. Whille it is possible to graphically show a single tree in the forest like in Figure \@ref(fig:rpartplot), it is not a statistically meaningful representation of the forest.

## Performance Evaluation

The RMSE of the four different models I have built so far are shown in Table \@ref(tab:rmse-table). All models are trained on the same training set and tested on the same test set. It is clear that the predictor transformation and selection algorithm I came up with for HW3 either paid off or got lucky with this test set - the simple multivariate linear regression model is the best performer out of the bunch. This is followed not so closely by random forest, then regression tree and finally the PCA MLR model. The performance of the regression tree surprised me since it is only capable of making two prediction values yet it did better than PCA MLR. Perhaps it's caused by the distribution of the response or the specific test set used.
\FloatBarrier
```{r rmse-table}
rmse_hw3 <- sqrt(1.3309e4)
rmse_q1 <- sqrt(q1_mse)
rmse_tree <- sqrt(tree_mse)
rmse_forest <- sqrt(forest_mse)

rmse_table <- data.frame(Model = c('HW3 MLR', 'Q1 PCA MLR', 'RPART Tree', 'Random Forest'), RMSE = c(rmse_hw3, rmse_q1, rmse_tree, rmse_forest))

kable(rmse_table, caption = 'RMSE of the 4 different models trained for the uscrime data.')
```
\pagebreak

# Question 3
**Describe a situation or problem from your job, everyday life, current events, etc., for which a logistic regression model would be appropriate. List some (up to 5) predictors that you might use.**

With the ISYE6501 mid-term approaching, many people are no doubt wondering if they will pass the test or not. A logistic regression model can be constructed to predict the outcome. The response is the success/fail of a student on his midterm test. Some of the predictors (assuming they can be systematically quantified) can be:

`Grades on Homework Assignments`

`Level of Participation in Office Hours`

`Grades on the Sample Quiz (before reading through the answers)`

`Time Spent on Preparation`

# Question 4
## Part 1
**Using the GermanCredit data set at http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german / (description at http://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29 ), use logistic regression to find a good predictive model for whether credit applicants are good credit risks or not. Show your model (factors used and their coefficients), the software output, and the quality of fit. You can use the glm function in R. To get a logistic regression (logit) model on data where the response is either zero or one, use family=binomial(link=”logit”) in your glm function call.**

```{r}
q4_data <- read.table('germancredit.txt')

q4_data$V21[q4_data$V21 == 2] = 0
    
q4_data$V21 <- factor(q4_data$V21, labels = c('Bad', 'Good'))

n <- nrow(q4_data)

inTrain <- sample(1:n, size = ceiling(n*0.8))

q4_data.train <- q4_data[inTrain,]
q4_data.test <- q4_data[-inTrain,]

# hack
q4_data.test <- rbind(q4_data.train[1,], q4_data.test)
q4_data.test <- q4_data.test[-1,]
```

The data is loaded and column 21 is identified as the categorical response. A bit of data manipulation is done to redefine "Bad" as 0 and "Good" as 1. The response is then converted to a factor.

```{r, results = 'hide'}
model <- glm(V21 ~., data = q4_data.train, family = binomial(link = 'logit'))
final_model <- stepAIC(model)
```

As usual the data is then split into a training (80%) and test (20%) set. A logistic regression model using all predictors is first constructed to allow backwards stepwise feature selection using `stepAIC()` from the `MASS` library. The final model statistics is shown below:

```{r}
summary(final_model)
```

It should be noted that some of the selected features display bad P-values. However they are all dummy variables created from the factors they belong to and as a whole one or more other levels in that particular factor are always found to be statistically significant. The final model can be written as:

```{r}
print_eq <- function(model) {
    pred_terms <- attributes(model$coefficients)$names
    coefficients <- model$coefficients
    
    equation <- paste('P = ', coefficients[1], ' + ', paste(mapply(paste, coefficients[2:length(pred_terms)], pred_terms[2:length(pred_terms)], sep = '*'), collapse = ' + '), sep = '')
    
    return(equation)
}

q4_eq <- print_eq(final_model)
```

$`r q4_eq`$

Notice that the model computes the *probability* of the response being positive (ie: credit should be classfied as "Good"), rather than the actual response labels themselves. 

### Performance Evaluation
Predictions are made using the logistic regression model on the test set. To convert predicted probabilities to the proper response labels, a sample cutoff is set at 0.5 (ie: P > 0.5 -> classify as "Good"). The resulting confusion matrix statistics are shown below:

```{r}
predict_q4 <- function(model, data, threshold = 0.5) {
    prob <- predict(model, data, type = 'response')
    pred <- factor(ifelse(prob > threshold, 'Good', 'Bad'))
    
    ans <- list(pred = pred, p = prob)
    return(ans)
}



pred.test <- predict_q4(final_model, q4_data.test, threshold = 0.5)

CM <- confusionMatrix(pred.test$pred, q4_data.test$V21, positive = 'Good')

CM
```

## Part 2
**Because the model gives a result between 0 and 1, it requires setting a threshold probability to separate between “good” and “bad” answers. In this data set, they estimate that incorrectly identifying a bad customer as good, is 5 times worse than incorrectly classifying a good customer as bad. Determine a good threshold probability based on your model.**

```{r}
cost_func <- function(CM, FP_cost = 5, FN_cost = 1) {
    cost <- CM$table[2,1]*FP_cost + CM$table[1,2]*FN_cost
}

wrapper <- function(model, data, threshold){
    pred <- predict_q4(model, data, threshold)
    CM <- caret::confusionMatrix(pred$pred, data$V21, positive = 'Good')
    cost <- cost_func(CM, 5, 1)
    return(cost)
}

grid_search <- function(model, data, range = c(0, 1), grid_n = 1001, level = 1) {
    thresholds <- seq(from = range[1], to = range[2], length.out = grid_n)
    n <- length(thresholds)
    
    cl <- makePSOCKcluster(4)
    clusterExport(cl = cl, varlist = list('predict_q4', 'cost_func'))

    costs <- parSapply(cl = cl, X = thresholds, FUN = wrapper, model = model, data = data)

    stopCluster(cl)
    
    min_ind <- which.min(costs)
    
    if (level < 2) {
        if (min_ind == 1) {
            select <- c(1, 3)
        } else if (min_ind == n) {
            select <- c(n - 2, n)
        } else {
            select <- c(min_ind - 1, min_ind + 1)
        }
        plot(thresholds, costs, pch ='.', xlab = 'Probability Threshold', ylab = 'Overall Cost')
        
        return(grid_search(model, data, range = thresholds[select], grid_n, level = 2))
        
    } else {
        ans <- list(threshold = thresholds[min_ind], cost = costs[min_ind])
        return(ans)
    }
}
```

A 2-nested grid search is carried out with 1000 intervals on each level to find the optimal threshold probability level that minimizes the cost on the test set. Figure \@ref(fig:cost-optimized) shows the cost as a function of threshold.

```{r cost-optimized, fig.cap = 'Cost vs Probability Threshold'}
optimized <- grid_search(final_model, q4_data.test)
```

The probability threshold P = `r optimized$threshold` results in the minimum cost `r optimized$cost`.